/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    sanger-tol/genomenote Nextflow base config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Increasing the number of CPUs often gives diminishing returns, so we increase it
    following a logarithm curve. Example:
        - 0 < value <= 1: start + step
        - 1 < value <= 2: start + 2*step
        - 2 < value <= 4: start + 3*step
        - 4 < value <= 8: start + 4*step
    In order to support re-runs, the step increase may be multiplied by the attempt
    number prior to calling this function.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

// Modified logarithm function that doesn't return negative numbers
def positive_log(value, base) {
    if (value <= 1) {
        return 0
    } else {
        return Math.log(value)/Math.log(base)
    }
}

def log_increase_cpus(start, step, value, base) {
    return check_max(start + step * (1 + Math.ceil(positive_log(value, base))), 'cpus')
}


process {

    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 5
    maxErrors     = '-1'

    // In this configuration file, we give little resources by default and
    // explicitly bump them up for some processes.
    // All rules should still increase resources every attempt to allow the
    // pipeline to self-heal from MEMLIMIT/RUNLIMIT.

    // Default
    cpus   = 1
    memory = { check_max( 50.MB * task.attempt, 'memory' ) }
    time   = { check_max( 30.min * task.attempt, 'time' ) }

    // These processes typically complete within 30 min to 1.5 hours.
    withName: 'BED_SORT|BEDTOOLS_BAMTOBED|COOLER_CLOAD|COOLER_ZOOMIFY|FILTER_BED' {
        time   = { check_max( 4.hour * task.attempt, 'time' ) }
    }

    // These processes may take a few hours.
    withName: 'FILTER_SORT|SAMTOOLS_VIEW' {
        time   = { check_max( 8.hour * task.attempt, 'time' ) }
    }

    withName: SAMTOOLS_VIEW {
        memory = { check_max( 1.GB  * task.attempt, 'memory'  ) }
    }

    withName: FASTK_FASTK {
        memory = { check_max( 12.GB * task.attempt, 'memory'  ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
    }

    withName: MERQURYFK_MERQURYFK {
        // Memory usage seems to be following two different linear rules:
        //  - 1 GB for every 60 Mbp for genomes < 840 Mbp
        //  - 2 GB for every 1 Gbp for genomes > 840 Mbp, with a 12 GB offset to match the previous rule
        memory = { check_max( 1.GB + ((meta.genome_size < 840000000) ? (Math.ceil(meta.genome_size/60000000) * 1.GB * task.attempt) : (Math.ceil(meta.genome_size/1000000000) * 2.GB * task.attempt + 12.GB)), 'memory' ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
    }

    withName: BUSCO {
        // Weird memory growth. The formula below is to fit the actual usage and avoid BUSCO being killed.
        memory = { check_max(1.GB * Math.max(Math.pow(2, positive_log(meta.genome_size/1000000, 10)+task.attempt),  Math.floor(meta.genome_size/1000000000) * 16 * task.attempt), 'cpus'  ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, Math.ceil(meta.genome_size/1000000000), 2) }
        time   = { check_max( 2.h * Math.ceil(meta.genome_size/1000000000) * task.attempt, 'time') }
    }

    withName: 'BED_SORT|FILTER_SORT' {
        cpus   = { log_increase_cpus(2, 2*task.attempt, 1, 2) }
        memory = { check_max( 16.GB * task.attempt, 'memory'  ) }
    }

    withName: COOLER_CLOAD {
        memory = { check_max( 6.GB  * task.attempt, 'memory'  ) }
    }

    withName: COOLER_DUMP {
        memory = { check_max( 100.MB * task.attempt, 'memory'  ) }
    }

    withName: COOLER_ZOOMIFY {
        cpus   = { log_increase_cpus(2, 2*task.attempt, 1, 2) }
        memory = { check_max( (meta.genome_size < 1000000000 ? 16.GB : 24.GB) * task.attempt, 'memory' ) }
    }

    withName: MULTIQC {
        memory = { check_max( 150.MB  * task.attempt, 'memory'  ) }
    }

    withName:CUSTOM_DUMPSOFTWAREVERSIONS {
        cache = false
    }
}
